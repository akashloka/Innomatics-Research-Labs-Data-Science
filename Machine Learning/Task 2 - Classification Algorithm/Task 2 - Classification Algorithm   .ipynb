{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0bb46f",
   "metadata": {},
   "source": [
    "## Author : Loka Akash Reddy\n",
    "### Innomatics Research Labs - Data Science October - 2021\n",
    "### Tasks - Machine Learning\n",
    "### Task 2 - Classification Algorithm\n",
    "### Classification on CIFAR 10 Dataset\n",
    "###  Kindly follow the instructions given in this document : https://docs.google.com/document/d/1598tSMcK-WvxrJdBC7GmJ0wrlFAxRxw4ifTo1z4AETE/edit?usp=sharing\n",
    "### Kindly checkout the jupyter notebook for more details: https://drive.google.com/file/d/1FSXSAWKOKcibqiRBmcWuXNMslaTv2dxn/view\n",
    "### Importing required libraries\n",
    "### Step - 1 (Setup Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec6dec",
   "metadata": {},
   "source": [
    "### Data Description:\n",
    "CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
    "\n",
    "The CIFAR-10 data consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images in the official data. We have preserved the train/test split from the original dataset.  The provided files are:\n",
    "\n",
    "train.7z - a folder containing the training images in png format\n",
    "\n",
    "test.7z - a folder containing the test images in png format\n",
    "\n",
    "trainLabels.csv - the training labels\n",
    "\n",
    "To discourage certain forms of cheating (such as hand labeling) we have added 290,000 junk images in the test set. These images are ignored in the scoring. We have also made trivial modifications to the official 10,000 test images to prevent looking them up by file hash. These modifications should not appreciably affect the scoring. You should predict labels for all 300,000 images.\n",
    "\n",
    "The label classes in the dataset are:\n",
    "\n",
    "- airplane \n",
    "- automobile \n",
    "- bird \n",
    "- cat \n",
    "- deer \n",
    "- dog \n",
    "- frog \n",
    "- horse \n",
    "- ship \n",
    "- truck\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a90824",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-229e900d7c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from builtins import range\n",
    "from six.moves import cPickle as pickle\n",
    "from imageio import imread\n",
    "import platform\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741df30",
   "metadata": {},
   "source": [
    "### Step - 2 (Download the dataset)\n",
    "### Use this code to download the dataset for the first time\n",
    "### You will be required to install wget using pip\n",
    "### For Linux users checkout the alternative for 'del'\n",
    "\n",
    "### !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
    "### !tar -xzvf cifar-10-python.tar.gz\n",
    "### !del cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328acec9",
   "metadata": {},
   "source": [
    "### Step - 3 (Load the Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ca748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86f6e0",
   "metadata": {},
   "source": [
    "### Step - 4 (Visualization)\n",
    "Next we visualize the CIFAR-10 dataset. Although these functions are being written for you, we highly recommend you go through the code and make yourself familiar as these are things you will be required to do very often when working on AI/ML projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6b890",
   "metadata": {},
   "source": [
    "### Step - 5 (Flattening the images)\n",
    "In the next cell we flatten each image into a single dimensional vector so that it is easy to process. You should be able to reason about the dimensions comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = 5000\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = 500\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b7",
   "metadata": {},
   "source": [
    "### Step - 6 (Apply all possible classification algorithms)\n",
    "For each algo complete the following steps with appropriate plots, diagrams and code -\n",
    "\n",
    "1. Discuss the intuition behind each algorithm\n",
    "2. Mention pros and cons\n",
    "3. Discuss about Model parameters and Hyperparameters\n",
    "4. Discuss about overfitting and underfitting with relavant plots and code (Hint: Use cross validation and plot hyperparameter vs accuracy score)\n",
    "5. Discuss about train and test time/space complexities\n",
    "6. Measure the performance of model using various metrics and write in detail about each metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc5745",
   "metadata": {},
   "source": [
    "### Step - 7 (Compare all the models)\n",
    "Conclude by following below mentioed steps -\n",
    "\n",
    "1. Plot a table showing all the algorithms used and corresponding performance metrics\n",
    "2. Is there any one algorithm that can be used on all the datasets? (Hint: Read about 'No Free Lunch' theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3b8c3",
   "metadata": {},
   "source": [
    "## Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
